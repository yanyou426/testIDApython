{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea6d6fae-fcd2-47f2-a7b1-1f4558b8dfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-30 06:26:16.161428: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['the', 'cat', 'sat', 'hat', 'in', 'with']\n",
      "[[0. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 2. 1. 1. 1. 1. 0.]\n",
      " [0. 2. 1. 0. 1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "docs = [\n",
    "  'the cat sat',\n",
    "  'the cat sat in the hat',\n",
    "  'the cat with the hat',\n",
    "]\n",
    "\n",
    "## Step 1: Determine the Vocabulary\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(docs)\n",
    "print(f'Vocabulary: {list(tokenizer.word_index.keys())}')\n",
    "\n",
    "## Step 2: Count\n",
    "vectors = tokenizer.texts_to_matrix(docs, mode='count')\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f99f72dd-e397-47b6-bf54-5b71a1906330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "string = 'b.eq'\n",
    "print(\".\" in string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0e5957-37da-4834-b03f-b8bcbcc5e3da",
   "metadata": {},
   "source": [
    "## Calculate bag of words vocabulary of three libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33d3abd4-68b8-49f6-9248-83772322ecb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 329\n",
      "['mov', 'add', 'cmp', 'ld', 'ldr', 'sub', 'jmp', 'push', 'lea', 'je', 'nop', 'call', 'test', 'movzx', 'jne', 'pop', 'b', 'daddiu', 'addiu', 'xor', 'lw', 'str', 'sd', 'move', 'daddu', 'and', 'sw', 'ret', 'movz', 'beqz', 'movdqa', 'shl', 'shr', 'movaps', 'bl', 'or', 'bnez', 'cbz', 'ldp', 'ldur', 'dext', 'dsll', 'lbu', 'ja', 'jb', 'ldrb', 'stp', 'bal', 'b.ne', 'inc', 'jalr', 'movn', 'jae', 'jbe', 'lhu', 'movdqu', 'ldrh', 'b.eq', 'lsl', 'palignr', 'sltu', 'andi', 'bne', 'sll', 'subu', 'adrp', 'sb', 'sh', 'strb', 'cbnz', 'jr', 'strh', 'dec', 'lsr', 'beq', 'sltiu', 'addu', 'lui', 'jle', 'movq', 'movsxd', 'dsubu', 'stur', 'subs', 'js', 'not', 'movups', 'lddqu', 'jge', 'lwu', 'csel', 'sllv', 'b.hi', 'jl', 'sar', 'b.ls', 'jg', 'eor', 'sbb', 'adc', 'neg', 'dsrl', 'sete', 'mul', 'vmovups', 'cmn', 'endbr32', 'orr', 'b.hs', 'cmove', 'endbr64', 'b.lo', 'por', 'setne', 'imul', 'cmovb', 'ud2', 'movntdq', 'slti', 'tbnz', 'dsllv', 'slt', 'movabs', 'cmova', 'cset', 'srl', 'pslldq', 'blr', 'b.le', 'psrldq', 'ori', 'bltz', 'cmovne', 'ccmp', 'cmovbe', 'ubfx', 'movsd', 'ldrsw', 'sxth', 'seh', 'div', 'asr', 'sdl', 'sdr', 'tbz', 'dsrlv', 'ldl', 'movk', 'ldurb', 'b.gt', 'cmovae', 'sxtw', 'leave', 'cpuid', 'prefetcht0', 'b.lt', 'srlv', 'prefetchnta', 'b.ge', 'jns', 'mvn', 'rep', 'fld', 'cdqe', 'ext', 'movsx', 'vmovaps', 'prefetcht1', 'fstp', 'srav', 'blez', 'mflo', 'movntq', 'tst', 'cmovge', 'nor', 'mfhi', 'shld', 'bic', 'seta', 'dsll32', 'xori', 'xorps', 'movsb', 'dsra', 'lb', 'setb', 'vmovntdq', 'bgez', 'sturb', 'shrd', 'movd', 'fmul', 'dmultu', 'psubd', 'b.mi', 'dmult', 'sfence', 'cmovg', 'ubfiz']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "bow_size = 200\n",
    "sum_opcode_string = []\n",
    "path = r\"./Dataset_zlib/*.json\"\n",
    "# path = r\"arm64-clang-O0_minigzipsh_acfg_disasm.json\"\n",
    "for input_json in glob.glob(path):\n",
    "    with open(input_json) as f_in:\n",
    "        idb_map = json.load(f_in)\n",
    "        if len(idb_map.keys()) != 1:\n",
    "            print(\"The keys of idb is {}\".format(idb_map.keys()))\n",
    "            break\n",
    "        # print(idb_map.keys())\n",
    "        info = {}\n",
    "        for key in idb_map.keys():\n",
    "            info = idb_map[key]\n",
    "            # print(info)\n",
    "            break\n",
    "        del info['arch']\n",
    "        # print(info.keys())\n",
    "        for func_add in info.keys():\n",
    "            # print(\"BoW of fuc{}: \".format(func_add))\n",
    "            opcode_string = \"\"\n",
    "            bb_info = info[func_add]\n",
    "            # print(bb_info)\n",
    "            for bb_add in bb_info['basic_blocks'].keys():\n",
    "                # tmp_dict = {}\n",
    "                single_bb_info = bb_info['basic_blocks'][bb_add]\n",
    "                opcode_list = single_bb_info['bb_mnems']\n",
    "                # print(opcode_list)\n",
    "                for string in opcode_list:\n",
    "                    if '.' in string:\n",
    "                        # print(string)\n",
    "                        # print(string.replace('.', 'replace'))\n",
    "                        opcode_string += (string.replace('.', 'replace') + \" \")\n",
    "                    else:\n",
    "                        opcode_string += (string + \" \")\n",
    "            sum_opcode_string.append(opcode_string)\n",
    "        \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sum_opcode_string)\n",
    "print(f'Vocabulary: {len(list(tokenizer.word_index.keys()))}')\n",
    "# print(f'Vocabulary size before replacing: {list(tokenizer.word_index.keys())}')\n",
    "# print(tokenizer.word_counts)\n",
    "\n",
    "voca = list(tokenizer.word_index.keys())\n",
    "for opcode in voca:\n",
    "    if \"replace\" in opcode:\n",
    "        voca[voca.index(opcode)] = opcode.replace('replace', '.')\n",
    "# print(voca)\n",
    "\n",
    "# voca_dict = tokenizer.word_counts\n",
    "# sort_voca_dict = sorted(voca_dict.items(),key = lambda x:x[1],reverse = True)\n",
    "# print(sort_voca_dict)\n",
    "\n",
    "res = voca[:bow_size]\n",
    "print(res)\n",
    "\n",
    "\n",
    "# vectors = tokenizer.texts_to_matrix(sum_opcode_string, mode='count')\n",
    "# print(vectors)\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c8295a32-a89c-4a0e-a3f0-434818acd97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d0b470f2-351b-4e31-a138-f15317cfc956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 162\n",
      "['mov', 'call', 'push', 'test', 'je', 'jmp', 'cmp', 'lea', 'add', 'jne', 'pop', 'xor', 'sub', 'movzx', 'ret', 'and', 'or', 'ja', 'nop', 'inc', 'movsxd', 'shl', 'cmove', 'movaps', 'shr', 'jle', 'jg', 'jb', 'setne', 'cmovne', 'movsx', 'movabs', 'jae', 'js', 'jbe', 'dec', 'imul', 'jl', 'sete', 'sbb', 'endbr64', 'endbr32', 'movups', 'bt', 'sar', 'jge', 'not', 'leave', 'jns', 'adc', 'repne', 'scasb', 'neg', 'xorps', 'idiv', 'cdqe', 'cmovg', 'movsd', 'cvtsi2sd', 'div', 'rep', 'cqo', 'cdq', 'mul', 'cmovge', 'cmovl', 'cmovs', 'fstp', 'seta', 'cmovae', 'cmova', 'movdqu', 'fild', 'cmovb', 'rol', 'setg', 'pxor', 'fld', 'cmovle', 'divsd', 'cmovbe', 'setb', 'bswap', 'stosd', 'movq', 'repe', 'cmpsb', 'cvttsd2si', 'fldcw', 'movdqa', 'movsb', 'setae', 'mulsd', 'shrd', 'pcmpeqd', 'cmovns', 'fistp', 'bts', 'setl', 'pshufd', 'fnstcw', 'punpcklqdq', 'ucomisd', 'shld', 'fdiv', 'sets', 'setbe', 'stosq', 'paddq', 'fxch', 'fdivp', 'setle', 'xchg', 'fmul', 'bsr', 'cwde', 'movapd', 'pand', 'ror', 'punpcklwd', 'movsq', 'setge', 'por', 'comisd', 'movd', 'punpcklbw', 'fldz', 'setns', 'fld1', 'fchs', 'psubq', 'btr', 'seto', 'fcomi', 'pcmpeqb', 'fcomip', 'fdivrp', 'fucomi', 'fmulp', 'punpckhwd', 'xorpd', 'fucomip', 'movhps', 'punpckldq', 'punpckhdq', 'psrlw', 'packuswb', 'pslld', 'stosb', 'punpckhqdq', 'psrlq', 'fimul', 'punpckhbw', 'pcmpgtd', 'psllq', 'shufpd', 'pcmpgtw', 'psllw', 'fst', 'fidivr', 'pcmpgtb', 'psrldq']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "bow_size = 200\n",
    "sum_opcode_string = []\n",
    "path = r\"./Dataset_curl/*.json\"\n",
    "# path = r\"arm64-clang-O0_minigzipsh_acfg_disasm.json\"\n",
    "for input_json in glob.glob(path):\n",
    "    with open(input_json) as f_in:\n",
    "        idb_map = json.load(f_in)\n",
    "        if len(idb_map.keys()) != 1:\n",
    "            print(\"The keys of idb is {}\".format(idb_map.keys()))\n",
    "            break\n",
    "        # print(idb_map.keys())\n",
    "        info = {}\n",
    "        for key in idb_map.keys():\n",
    "            info = idb_map[key]\n",
    "            # print(info)\n",
    "            break\n",
    "        del info['arch']\n",
    "        # print(info.keys())\n",
    "        for func_add in info.keys():\n",
    "            # print(\"BoW of fuc{}: \".format(func_add))\n",
    "            opcode_string = \"\"\n",
    "            bb_info = info[func_add]\n",
    "            # print(bb_info)\n",
    "            for bb_add in bb_info['basic_blocks'].keys():\n",
    "                # tmp_dict = {}\n",
    "                single_bb_info = bb_info['basic_blocks'][bb_add]\n",
    "                opcode_list = single_bb_info['bb_mnems']\n",
    "                # print(opcode_list)\n",
    "                for string in opcode_list:\n",
    "                    if '.' in string:\n",
    "                        # print(string)\n",
    "                        # print(string.replace('.', 'replace'))\n",
    "                        opcode_string += (string.replace('.', 'replace') + \" \")\n",
    "                    else:\n",
    "                        opcode_string += (string + \" \")\n",
    "            sum_opcode_string.append(opcode_string)\n",
    "        \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sum_opcode_string)\n",
    "print(f'Vocabulary: {len(list(tokenizer.word_index.keys()))}')\n",
    "# print(f'Vocabulary size before replacing: {list(tokenizer.word_index.keys())}')\n",
    "# print(tokenizer.word_counts)\n",
    "\n",
    "voca = list(tokenizer.word_index.keys())\n",
    "\n",
    "for opcode in voca:\n",
    "    if \"replace\" in opcode:\n",
    "        voca[voca.index(opcode)] = opcode.replace('replace', '.')\n",
    "print(voca)\n",
    "\n",
    "# voca_dict = tokenizer.word_counts\n",
    "# sort_voca_dict = sorted(voca_dict.items(),key = lambda x:x[1],reverse = True)\n",
    "# print(sort_voca_dict)\n",
    "\n",
    "# res = voca[:bow_size]\n",
    "# print(res)\n",
    "\n",
    "\n",
    "# vectors = tokenizer.texts_to_matrix(sum_opcode_string, mode='count')\n",
    "# print(vectors)\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fe4778c6-e6b1-4eb8-b549-e0ba6f0f29c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 625\n",
      "['mov', 'ld', 'push', 'add', 'call', 'move', 'ldr', 'daddiu', 'nop', 'sd', 'cmp', 'lea', 'bl', 'b', 'test', 'xor', 'je', 'jalr', 'movz', 'jmp', 'sub', 'pop', 'addiu', 'str', 'beqz', 'jne', 'adrp', 'ldp', 'cbz', 'ret', 'lw', 'stp', 'and', 'daddu', 'bnez', 'sw', 'ldur', 'bal', 'eor', 'cbnz', 'jr', 'movzx', 'b.eq', 'pxor', 'ror', 'b.ne', 'or', 'lui', 'stur', 'movdqa', 'sll', 'ldrb', 'jle', 'lbu', 'shr', 'beq', 'bne', 'dsll', 'andi', 'sb', 'shl', 'movups', 'dext', 'orr', 'movsxd', 'strb', 'adc', 'inc', 'movn', 'endbr64', 'lsr', 'movdqu', 'addu', 'jg', 'ja', 'sltu', 'lsl', 'subs', 'tbnz', 'blez', 'blr', 'sltiu', 'endbr32', 'jb', 'vpxor', 'rol', 'b.lt', 'mul', 'b.le', 'slt', 'cset', 'paddd', 'csel', 'js', 'sxtw', 'srl', 'jl', 'slti', 'movq', 'b.gt', 'dsubu', 'dec', 'rorx', 'ori', 'jbe', 'pand', 'shrd', 'rotr', 'dsrl', 'setne', 'jae', 'b.hi', 'ldrsw', 'paddq', 'jge', 'bswap', 'sar', 'vpaddd', 'xorps', 'movaps', 'psrlq', 'movabs', 'vmovdqu', 'bltz', 'sete', 'ubfx', 'tbz', 'psrld', 'aesenc', 'subu', 'cmove', 'por', 'pshufd', 'b.ls', 'imul', 'sbb', 'lb', 'b.lo', 'bgtz', 'psllq', 'not', 'ext', 'ccmp', 'b.ge', 'neg', 'movk', 'vpsrld', 'cdqe', 'pslld', 'vpaddq', 'lwu', 'cmovne', 'leave', 'movsx', 'b.hs', 'cmn', 'dsll32', 'jns', 'vmovdqa', 'rep', 'sra', 'aesdec', 'tst', 'vpslld', 'bgez', 'adcs', 'drotr', 'movd', 'ldl', 'br', 'sdl', 'sdr', 'asr', 'drotr32', 'shld', 'fmov', 'bic', 'movsd', 'rev', 'pshufb', 'mflo', 'umulh', 'adds', 'vpmuludq', 'ldrh', 'vpsrlq', 'dsrl32', 'sturb', 'ld1', 'lhu', 'ldurb', 'seta', 'fstp', 'aese', 'ins', 'cmovns', 'adcx', 'bt', 'mfhi', 'madd']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "bow_size = 200\n",
    "sum_opcode_string = []\n",
    "path = r\"./Dataset_openssl/*.json\"\n",
    "# path = r\"arm64-clang-O0_minigzipsh_acfg_disasm.json\"\n",
    "for input_json in glob.glob(path):\n",
    "    with open(input_json) as f_in:\n",
    "        idb_map = json.load(f_in)\n",
    "        if len(idb_map.keys()) != 1:\n",
    "            print(\"The keys of idb is {}\".format(idb_map.keys()))\n",
    "            break\n",
    "        # print(idb_map.keys())\n",
    "        info = {}\n",
    "        for key in idb_map.keys():\n",
    "            info = idb_map[key]\n",
    "            # print(info)\n",
    "            break\n",
    "        del info['arch']\n",
    "        # print(info.keys())\n",
    "        for func_add in info.keys():\n",
    "            # print(\"BoW of fuc{}: \".format(func_add))\n",
    "            opcode_string = \"\"\n",
    "            bb_info = info[func_add]\n",
    "            # print(bb_info)\n",
    "            for bb_add in bb_info['basic_blocks'].keys():\n",
    "                # tmp_dict = {}\n",
    "                single_bb_info = bb_info['basic_blocks'][bb_add]\n",
    "                opcode_list = single_bb_info['bb_mnems']\n",
    "                # print(opcode_list)\n",
    "                for string in opcode_list:\n",
    "                    if '.' in string:\n",
    "                        # print(string)\n",
    "                        # print(string.replace('.', 'replace'))\n",
    "                        opcode_string += (string.replace('.', 'replace') + \" \")\n",
    "                    else:\n",
    "                        opcode_string += (string + \" \")\n",
    "            sum_opcode_string.append(opcode_string)\n",
    "        \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sum_opcode_string)\n",
    "print(f'Vocabulary: {len(list(tokenizer.word_index.keys()))}')\n",
    "# print(f'Vocabulary size before replacing: {list(tokenizer.word_index.keys())}')\n",
    "# print(tokenizer.word_counts)\n",
    "\n",
    "voca = list(tokenizer.word_index.keys())\n",
    "for opcode in voca:\n",
    "    if \"replace\" in opcode:\n",
    "        voca[voca.index(opcode)] = opcode.replace('replace', '.')\n",
    "# print(voca)\n",
    "\n",
    "# voca_dict = tokenizer.word_counts\n",
    "# sort_voca_dict = sorted(voca_dict.items(),key = lambda x:x[1],reverse = True)\n",
    "# print(sort_voca_dict)\n",
    "\n",
    "res = voca[:bow_size]\n",
    "print(res)\n",
    "\n",
    "\n",
    "# vectors = tokenizer.texts_to_matrix(sum_opcode_string, mode='count')\n",
    "# print(vectors)\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24cb219-32ca-441e-a3a1-4a2e669a8d40",
   "metadata": {},
   "source": [
    "## Calculate TF-IDF of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16fffd4-ea77-4b67-83b3-36f3c661acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "notewhorty_words_set = pickle.load( open( \"notewhorty_words.p\", \"rb\" ) )\n",
    "notewhorty_words = sorted(list(notewhorty_words_set))\n",
    "notewhorty_words_indexes = {}\n",
    "for i, word in enumerate(notewhorty_words):\n",
    "    notewhorty_words_indexes[word] = i\n",
    "\n",
    "\n",
    "def increment_word(dict, key):\n",
    "    if key not in dict:\n",
    "        dict[key] = 1\n",
    "    else:\n",
    "        dict[key] += 1\n",
    "\n",
    "def create_words_count(dataset_name):\n",
    "\n",
    "    total = 0\n",
    "    for path in Path(data_folder_name).glob('**/*.txt'):\n",
    "        total += 1\n",
    "\n",
    "    file = open(dataset_name + \".pv\", \"w\")\n",
    "\n",
    "    with tqdm(total=total) as pbar:\n",
    "        for path in Path(data_folder_name).glob('**/*.txt'):\n",
    "            document = [0 for _ in notewhorty_words]\n",
    "            f = open(str(path), \"r\")\n",
    "            text = f.read()\n",
    "            f.close()\n",
    "            file.write(str(path))\n",
    "            file.write(\" \")\n",
    "            text = text.lower()\n",
    "            text = ''.join(c for c in text if c not in string.punctuation)\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                if word in notewhorty_words_set:\n",
    "                    document[notewhorty_words_indexes[word]] += 1\n",
    "            for word in document:\n",
    "                file.write(str(word))\n",
    "                file.write(\" \")\n",
    "            for target in get_target(str(path)):\n",
    "                file.write(str(target))\n",
    "                file.write(\" \")\n",
    "            file.write('\\n')\n",
    "            pbar.update(1)\n",
    "\n",
    "\n",
    "def create_tfidf(dataset_name):\n",
    "\n",
    "    total = 0\n",
    "    for path in Path(data_folder_name).glob('**/*.txt'):\n",
    "        total += 1\n",
    "\n",
    "    file = open(dataset_name + \".pv\", \"w\")\n",
    "\n",
    "    tokenizer_obj = Tokenizer()\n",
    "    #tokenizer_obj.fit_on_texts(notewhorty_words_set)\n",
    "\n",
    "    logging.info(\"FITTING WORDS\")\n",
    "    with tqdm(total=total) as pbar:\n",
    "        for path in Path(data_folder_name).glob('**/*.txt'):\n",
    "            document = [0 for _ in notewhorty_words]\n",
    "            f = open(str(path), \"r\")\n",
    "            text = f.read()\n",
    "            f.close()\n",
    "            text = text.lower()\n",
    "            text = ''.join(c for c in text if c not in string.punctuation)\n",
    "            words = text.split()\n",
    "            new_text = []\n",
    "            for word in words:\n",
    "                if word in notewhorty_words_set:\n",
    "                    new_text.append(word)\n",
    "            tokenizer_obj.fit_on_texts(new_text)\n",
    "            pbar.update(1)\n",
    "\n",
    "    logging.info(\"COMPUTE TF-IDF WORDS\")\n",
    "    with tqdm(total=total) as pbar:\n",
    "        for path in Path(data_folder_name).glob('**/*.txt'):\n",
    "            document = [0 for _ in notewhorty_words]\n",
    "            f = open(str(path), \"r\")\n",
    "            text = f.read()\n",
    "            f.close()\n",
    "            file.write(str(path))\n",
    "            file.write(\" \")\n",
    "            text = text.lower()\n",
    "            text = ''.join(c for c in text if c not in string.punctuation)\n",
    "            words = text.split()\n",
    "            new_text = []\n",
    "            for word in words:\n",
    "                if word in notewhorty_words_set:\n",
    "                    new_text.append(word)\n",
    "            sequence = tokenizer_obj.texts_to_sequences([new_text])\n",
    "            document = tokenizer_obj.sequences_to_matrix(sequence, mode='tfidf')\n",
    "            for word in document[0]:\n",
    "                file.write(str(word))\n",
    "                file.write(\" \")\n",
    "            for target in get_target(str(path)):\n",
    "                file.write(str(target))\n",
    "                file.write(\" \")\n",
    "            file.write('\\n')\n",
    "            pbar.update(1)\n",
    "\n",
    "\n",
    "def get_target(p):\n",
    "    result = [0,0,0,0]\n",
    "    if 'TRIVIAL' in p:\n",
    "        result[0] = 1\n",
    "    if 'STRING_ENCRY' in p:\n",
    "        result[1] = 1\n",
    "    if 'REFLECTION' in p:\n",
    "        result[2] = 1\n",
    "    if 'CLASS_ENCRYPTION' in p:\n",
    "        result[3] = 1\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # create_tfidf(\"dataset_tfidf\")\n",
    "    create_words_count(\"dataset_words_count\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
